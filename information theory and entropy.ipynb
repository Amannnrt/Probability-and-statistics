{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b95c27-b773-458b-8ac1-7116264d84a2",
   "metadata": {},
   "source": [
    "# Information Theory  \n",
    "\n",
    "## What is Information?  \n",
    "In information theory, information is closely tied to the concept of *surprise*.  \n",
    "\n",
    "- If an event is **unlikely**, learning that it happened carries **more information** because it is more surprising.  \n",
    "- On the other hand, if an event is **certain** (i.e., it was guaranteed to happen), it carries **zero information** because it is not surprising at all.  \n",
    "\n",
    "Mathematically, we measure the information content of an event \\( x \\) using the formula:  \n",
    "$$\n",
    "h(x) = -\\log p(x),\n",
    "$$\n",
    "where:  \n",
    "- $p(x) $ is the probability of the event $ x $  \n",
    "- $ h(x) $ represents the amount of information (or surprise) associated with $ x $ \n",
    "\n",
    "---\n",
    "\n",
    "## Connection to Entropy  \n",
    "\n",
    "The entropy of a discrete probability distribution represents the *expected information content*.  \n",
    "It measures the **average amount of surprise** we can expect when observing outcomes from that distribution.  \n",
    "\n",
    "Mathematically, entropy is defined as:  \n",
    "$$\n",
    "H(X) = \\mathbb{E}[h(x)] = - \\sum_{x} p(x) \\log p(x),\n",
    "$$\n",
    "where:  \n",
    "- $ H(X)$ is the entropy,  \n",
    "- $ p(x) $ is the probability of event x,  \n",
    "- The summation is taken over all possible outcomes x.  \n",
    "\n",
    "---\n",
    "\n",
    "## Entropy and Probability Distribution  \n",
    "\n",
    "Entropy depends on how the probability is distributed across different outcomes:  \n",
    "\n",
    "- If the probability distribution is **sharply peaked** (i.e., most of the probability is concentrated around a few values), the entropy is **low**.  \n",
    "- If the probability distribution is more **spread out** (i.e., all outcomes are roughly equally likely), the entropy is **high**.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![image5](image5.png)\n",
    "---\n",
    "## Maximum Entropy  \n",
    "The maximum possible entropy occurs for a **uniform distribution**, where all outcomes are equally likely.  \n",
    "This is because there is the **most uncertainty** in predicting the outcome when each event has the same probability.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493a9e2-3f69-4787-a345-dc60fb59c6e9",
   "metadata": {},
   "source": [
    "# Finding the Maximum Entropy Configuration\n",
    "\n",
    "The maximum entropy configuration can be found by maximizing the entropy \\( H \\), subject to the normalization constraint on the probabilities \\( p(x_i) \\). Letâ€™s solve this step by step.\n",
    "\n",
    "## Step 1: Define the Entropy Function\n",
    "The entropy  H  is defined as:\n",
    "$\n",
    "H = - \\sum_{i} p(x_i) \\ln p(x_i),\n",
    "$\n",
    "where \\( p(x_i) \\) represents the probability of the \\( i \\)-th event.\n",
    "\n",
    "## Step 2: Define the Constraint\n",
    "We have the following constraint:\n",
    "$\n",
    "\\sum_{i} p(x_i) = 1.\n",
    "$\n",
    "This means the total probability across all possible outcomes must equal 1.\n",
    "\n",
    "## Step 3: Set Up the Lagrange Function\n",
    "To maximize the entropy H  under the constraint $ \\sum_{i} p(x_i) = 1 $, we use the method of **Lagrange multipliers**.  \n",
    "We define the Lagrange function \\( \\mathcal{L} \\) as:\n",
    "$$\n",
    "\\mathcal{L}(p(x_i), \\lambda) = - \\sum_{i} p(x_i) \\ln p(x_i) + \\lambda \\left( \\sum_{i} p(x_i) - 1 \\right),\n",
    "$$\n",
    "where $ \\lambda $ is the Lagrange multiplier that enforces the constraint $ \\sum_{i} p(x_i) = 1 $.\n",
    "\n",
    "## Step 4: Differentiate the Lagrange Function\n",
    "Now, we want to maximize $ \\mathcal{L} $. To do this, we take the partial derivative of $ \\mathcal{L} $ with respect to each $ p(x_i) $ and set it equal to 0.\n",
    "\n",
    "### Differentiate:\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p(x_i)} = - \\left( \\ln p(x_i) + 1 \\right) + \\lambda.\n",
    "$\n",
    "\n",
    "Set this equal to 0:\n",
    "\n",
    "$\n",
    "\\left( \\ln p(x_i) + 1 \\right) + \\lambda = 0.\n",
    "$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$\n",
    "\\ln p(x_i) = \\lambda - 1.\n",
    "$\n",
    "\n",
    "## Step 5: Solve for $ p(x_i) $\n",
    "To solve for \\( p(x_i) \\), take the exponential of both sides:\n",
    "$$\n",
    "p(x_i) = e^{\\lambda - 1}.\n",
    "$$\n",
    "Since this must hold for all \\( i \\), we have:\n",
    "$$\n",
    "p(x_1) = p(x_2) = \\dots = p(x_n) = e^{\\lambda - 1}.\n",
    "$$\n",
    "In other words, all the probabilities are equal.\n",
    "\n",
    "## Step 6: Use the Normalization Condition\n",
    "From the normalization condition $ \\sum_{i} p(x_i) = 1 $, and since there are  n  possible outcomes:\n",
    "$$\n",
    "n \\cdot p(x_i) = 1.\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "p(x_i) = \\frac{1}{n}.\n",
    "$$\n",
    "\n",
    "## Step 7: Conclusion - Maximum Entropy Configuration\n",
    "The maximum entropy configuration occurs when all probabilities are equal, i.e., when:\n",
    "$$\n",
    "p(x_i) = \\frac{1}{n}.\n",
    "$$\n",
    "This corresponds to a **uniform distribution**.\n",
    "\n",
    "## Final Interpretation:\n",
    "When all outcomes are equally likely, we have the greatest uncertainty (or the highest entropy). This is why the uniform distribution maximizes entropy under the given constraint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f87663-d4a8-4fc8-a3de-f0920cf669e4",
   "metadata": {},
   "source": [
    "# Differential Entropy and Maximum Entropy Configuration\n",
    "\n",
    "### Differential Entropy\n",
    "For continuous probability distributions, the **differential entropy** is given by:\n",
    "\n",
    "$$\n",
    "H[x] = - \\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx. \\tag{1.104}\n",
    "$$\n",
    "\n",
    "### Maximum Entropy for Continuous Variables\n",
    "In the case of discrete distributions, we saw that the maximum entropy configuration corresponds to an equal distribution of probabilities across the possible states of the variable.\n",
    "\n",
    "Now, let us consider the maximum entropy configuration for a continuous variable.  \n",
    "To ensure that the maximum is well-defined, it is necessary to constrain the **first and second moments** of \\( p(x) \\), while also preserving the **normalization constraint**.\n",
    "\n",
    "Thus, we maximize the differential entropy subject to the following constraints:\n",
    "\n",
    "1. **Normalization Constraint:**\n",
    "   $$\n",
    "   \\int_{-\\infty}^{\\infty} p(x) \\, dx = 1 \\tag{1.105}\n",
    "   $$\n",
    "\n",
    "2. **Constraint on the Mean:**\n",
    "   $$\n",
    "   \\int_{-\\infty}^{\\infty} x p(x) \\, dx = \\mu \\tag{1.106}\n",
    "   $$\n",
    "\n",
    "3. **Constraint on the Variance:**\n",
    "   $$\n",
    "   \\int_{-\\infty}^{\\infty} (x - \\mu)^2 p(x) \\, dx = \\sigma^2 \\tag{1.107}\n",
    "   $$\n",
    "\n",
    "### Constrained Maximization with Lagrange Multipliers\n",
    "To perform the constrained maximization, we introduce **Lagrange multipliers** and maximize the following functional with respect to \\( p(x) \\):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx \n",
    "+ \\lambda_1 \\left( \\int_{-\\infty}^{\\infty} p(x) \\, dx - 1 \\right)\n",
    "+ \\lambda_2 \\left( \\int_{-\\infty}^{\\infty} x p(x) \\, dx - \\mu \\right)\n",
    "+ \\lambda_3 \\left( \\int_{-\\infty}^{\\infty} (x - \\mu)^2 p(x) \\, dx - \\sigma^2 \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069504d4-9c66-45f7-867e-3d76a691c3f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 1: Functional Derivative\n",
    "\n",
    "To maximize \\( \\mathcal{L} \\), we take the functional derivative $ \\frac{\\delta \\mathcal{L}}{\\delta p(x)} $ and set it equal to 0. We will now differentiate each term:\n",
    "\n",
    "### 1. First Term:  \n",
    "$$\n",
    "\\frac{\\delta}{\\delta p(x)} \\left( - \\int_{-\\infty}^{\\infty} p(x) \\ln p(x) dx \\right)\n",
    "= -( \\ln p(x) + 1 )\n",
    "$$\n",
    "\n",
    "### 2. Second Term (Normalization constraint):  \n",
    "$$\n",
    "\\frac{\\delta}{\\delta p(x)} \\left( \\lambda_1 \\left( \\int_{-\\infty}^{\\infty} p(x) dx - 1 \\right) \\right)\n",
    "= \\lambda_1\n",
    "$$\n",
    "\n",
    "### 3. Third Term (Mean constraint):  \n",
    "$$\n",
    "\\frac{\\delta}{\\delta p(x)} \\left( \\lambda_2 \\int_{-\\infty}^{\\infty} x p(x) dx \\right)\n",
    "= \\lambda_2 x\n",
    "$$\n",
    "\n",
    "### 4. Fourth Term (Variance constraint):  \n",
    "Expand \\( (x - \\mu)^2 \\) inside the integral:\n",
    "$$\n",
    "\\frac{\\delta}{\\delta p(x)} \\left( \\lambda_3 \\int_{-\\infty}^{\\infty} (x - \\mu)^2 p(x) dx \\right)\n",
    "= \\lambda_3 (x - \\mu)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Setting up the Functional Derivative\n",
    "\n",
    "Summing all the partial derivatives, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta p(x)} = - \\left( \\ln p(x) + 1 \\right) + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x - \\mu)^2\n",
    "$$\n",
    "\n",
    "Set this equal to 0:\n",
    "\n",
    "$$\n",
    "\\left( \\ln p(x) + 1 \\right) + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Solving for \\( p(x) \\)\n",
    "\n",
    "Rearrange the equation:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = - 1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x - \\mu)^2\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = C + \\lambda_2 x + \\lambda_3 (x - \\mu)^2\n",
    "$$\n",
    "where $( C = -1 + \\lambda_1 )$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Exponentiate Both Sides\n",
    "\n",
    "To isolate p(x) , exponentiate both sides:\n",
    "\n",
    "$$\n",
    "p(x) = \\exp \\left( C + \\lambda_2 x + \\lambda_3 (x - \\mu)^2 \\right)\n",
    "$$\n",
    "\n",
    "Since $ \\exp(A + B) = \\exp(A) \\cdot \\exp(B) $, we can write:\n",
    "\n",
    "$$\n",
    "p(x) = A \\cdot \\exp \\left( \\lambda_2 x + \\lambda_3 (x - \\mu)^2 \\right)\n",
    "$$\n",
    "where $ A = e^C $.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Identifying the Solution as a Gaussian Distribution\n",
    "\n",
    "Notice that the expression inside the exponential has a quadratic form $ (x - \\mu)^2 $, which is characteristic of a **Gaussian distribution**.\n",
    "\n",
    "By choosing appropriate values for $ \\lambda_2  and \\lambda_3 $ , we get the standard form of the Gaussian:\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Final Answer:\n",
    "The maximum entropy distribution, subject to the constraints on mean and variance, is a **Gaussian distribution**:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed65a9-ded5-429d-9228-475d989cc5d3",
   "metadata": {},
   "source": [
    "The entropy of a probability distribution measures the uncertainty or randomness associated with that distribution. A higher entropy means greater uncertainty in predicting the outcome.\n",
    "\n",
    "By maximizing entropy, you are essentially selecting the least biased distribution that satisfies the constraints you impose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d74585-0bda-48d2-b3f4-a38564e4dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
