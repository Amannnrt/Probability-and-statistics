{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d9bd8b-3bd2-4797-823b-a31ef11fb21c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Predictive Distribution\n",
    "\n",
    "The predictive distribution we aim to understand is:\n",
    "\n",
    "$\n",
    "p(t \\mid x, x_{\\text{train}}, t_{\\text{train}}) = \\int p(t \\mid x, w) \\, p(w \\mid x_{\\text{train}}, t_{\\text{train}}) \\, dw,\n",
    "$\n",
    "where:\n",
    "-  x  is the input for a **new test point** (the data point where we want to predict the target \\( t \\)).\n",
    "- $ x_{\\text{train}} $ and $ t_{\\text{train}} $ are the inputs and outputs from the **training data** we’ve already observed.\n",
    "- w  is the vector of **model parameters** (for example, the polynomial coefficients in polynomial regression).\n",
    "- $ p(t \\mid x, w) $ is the **likelihood**, which tells us how likely a target value  t is, given the input x  and a particular set of parameters $ w $.\n",
    "- $ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) $ is the **posterior distribution** over $ w $, which reflects our updated belief about the parameters $ w $, after observing the training data.\n",
    "\n",
    "### Understanding the Integral\n",
    "\n",
    "The integral sums over all possible values of  w , and each value of  w  is weighted by its posterior probability $ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) $ . This process of **averaging over the parameters** is called **marginalization**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Are We Integrating Over $ w $ ?\n",
    "\n",
    "In Bayesian reasoning, we don’t just want a single \"best guess\" for the parameters  w . Instead, we recognize that there may be many plausible values of $ w $, each of which might give a slightly different prediction for the target value t . \n",
    "\n",
    "To make a robust prediction, we average over all possible values of  w , with each value weighted by how likely it is, according to the posterior distribution $ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) $. \n",
    "\n",
    "This gives us a **probabilistic prediction** that incorporates both the likelihood and the uncertainty about  w. \n",
    "\n",
    "---\n",
    "\n",
    "## Breaking Down the Components\n",
    "\n",
    "Let’s break down each term in the predictive distribution:\n",
    "\n",
    "1. **Likelihood: $ p(t \\mid x, w) $**  \n",
    "   This tells us the probability of observing a target value \\( t \\) for a given input \\( x \\), assuming a specific set of parameters \\( w \\). For example, in linear regression, this could be a Gaussian distribution centered around the predicted output \\( y(x, w) \\).\n",
    "\n",
    "2. **Posterior: $ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) $**  \n",
    "   This reflects our belief about the parameters \\( w \\), after observing the training data. It combines the prior distribution over \\( w \\) and the likelihood of the observed training data.\n",
    "\n",
    "3. **Integral (Marginalization):**  \n",
    "   The integral:\n",
    "\n",
    "   $$\n",
    "   \\int p(t \\mid x, w) \\, p(w \\mid x_{\\text{train}}, t_{\\text{train}}) \\, dw,\n",
    "   $$\n",
    "   averages the predictions from all possible parameter values, weighted by their posterior probabilities. This accounts for our uncertainty about\n",
    "\n",
    "   $ w $ and produces a more robust and probabilistic prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## An Intuitive Analogy: Predicting Tomorrow’s Weather\n",
    "\n",
    "Imagine you’re trying to predict the temperature tomorrow based on historical data. Different weather models (analogous to different values of$ w $) might give slightly different predictions, and each model might have a different level of confidence. \n",
    "\n",
    "For example:\n",
    "- One model predicts 25°C with 60% confidence.\n",
    "- Another model predicts 22°C with 30% confidence.\n",
    "- A third model predicts 20°C with 10% confidence.\n",
    "\n",
    "Instead of picking just one model, you could **average their predictions**, weighted by how confident you are in each one. This might give you a final prediction like:\n",
    "\n",
    "$\n",
    "\\text{Expected temperature: 24°C}, \\quad \\text{with some uncertainty (because the models don’t all agree)}.\n",
    "$\n",
    "\n",
    "This is exactly what the integral is doing in Bayesian inference! It averages the predictions \\( p(t \\mid x, w) \\), weighted by the posterior probabilities $ p(w \\mid x_{\\text{train}}, t_{\\text{train}})$.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Is This Important?\n",
    "\n",
    "This Bayesian approach has several key advantages:\n",
    "- **Uncertainty Quantification:** The predictive distribution not only gives us a predicted value for \\( t \\), but also provides a measure of uncertainty, which can be useful in decision-making.\n",
    "- **Robust Predictions:** By averaging over many possible values of \\( w \\), we avoid overfitting to a single set of parameters.\n",
    "- **Flexibility:** The Bayesian framework can be extended to more complex models and priors, allowing us to incorporate domain-specific knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9adb0d-58a1-446e-a806-9dcfb9df70a2",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression: Posterior Distribution\n",
    "\n",
    "### Posterior Probability\n",
    "The posterior distribution of the weights $w$ given the training data $(x_{\\text{train}}, t_{\\text{train}})$ is given by:  \n",
    "$ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) \\propto p(t_{\\text{train}} \\mid x_{\\text{train}}, w) p(w) $  \n",
    "\n",
    "where:\n",
    "- $ p(t_{\\text{train}} \\mid x_{\\text{train}}, w) $: Likelihood of the training outputs $t_{\\text{train}}$ given the inputs $x_{\\text{train}}$ and the weights $w$.\n",
    "- $ p(w) $: Prior distribution over the weights $w$.\n",
    "\n",
    "### Likelihood Function\n",
    "The likelihood is given by:  \n",
    "$$ p(t_{\\text{train}} \\mid x_{\\text{train}}, w) = \\prod_{n=1}^{N} p(t_n \\mid x_n, w) = \\prod_{n=1}^{N} \\mathcal{N}(t_n \\mid w^T \\phi(x_n), \\beta^{-1}) $$ \n",
    "\n",
    "In exponential form:  \n",
    "$$ p(t_{\\text{train}} \\mid x_{\\text{train}}, w) \\propto \\exp\\left(-\\frac{\\beta}{2} \\| t - \\Phi w \\|^2 \\right) $$\n",
    "\n",
    "### Prior Distribution\n",
    "We assume a Gaussian prior on $w$:  \n",
    "$ p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I) $  \n",
    "\n",
    "This prior has mean $0$ and covariance matrix $\\alpha^{-1} I$, where $\\alpha$ is the precision of the prior. In explicit form:  \n",
    "$$p(w) \\propto \\exp\\left(-\\frac{\\alpha}{2} w^T w\\right) $$\n",
    "\n",
    "### Posterior Derivation\n",
    "Using Bayes' theorem, we combine the likelihood and prior. Since both are Gaussian distributions, the posterior is also Gaussian:  \n",
    "$ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) = \\mathcal{N}(w \\mid m_N, S_N) $  \n",
    "\n",
    "where we need to determine the posterior mean $m_N$ and posterior covariance $S_N$.\n",
    "\n",
    "### Completing the Square\n",
    "To derive $m_N$ and $S_N$, we start with:  \n",
    "$ p(w \\mid x_{\\text{train}}, t_{\\text{train}}) \\propto \\exp\\left(-\\frac{1}{2} \\left[\\beta (t - \\Phi w)^T (t - \\Phi w) + \\alpha w^T w \\right] \\right) $  \n",
    "\n",
    "Expand the quadratic term:  \n",
    "$ \\beta (t - \\Phi w)^T (t - \\Phi w) = \\beta \\left[t^T t - 2 w^T \\Phi^T t + w^T \\Phi^T \\Phi w \\right] $  \n",
    "\n",
    "Add the prior term:  \n",
    "$ \\beta w^T \\Phi^T \\Phi w + \\alpha w^T w - 2 \\beta w^T \\Phi^T t $  \n",
    "\n",
    "Group the quadratic and linear terms:  \n",
    "$ w^T (\\beta \\Phi^T \\Phi + \\alpha I) w - 2 \\beta w^T \\Phi^T t $\n",
    "\n",
    "### Completing the Square\n",
    "Reorganize the expression:  \n",
    "$$ -\\frac{1}{2} \\left[ w^T A w - 2 w^T b \\right] $$ \n",
    "\n",
    "where:\n",
    "- $ A = \\beta \\Phi^T \\Phi + \\alpha I $  \n",
    "- $ b = \\beta \\Phi^T t $  \n",
    "\n",
    "Add and subtract $ b^T A^{-1} b $:  \n",
    "$$ -\\frac{1}{2} \\left[ w^T A w - 2 w^T b + b^T A^{-1} b - b^T A^{-1} b \\right] $$\n",
    "\n",
    "This can be split into two terms:  \n",
    "$$ -\\frac{1}{2} \\left[ (w^T A w - 2 w^T b + b^T A^{-1} b) - b^T A^{-1} b \\right] $$\n",
    "\n",
    "Rewrite the perfect square term:  \n",
    "$$  w^T A w - 2 w^T b + b^T A^{-1} b = (w - A^{-1} b)^T A (w - A^{-1} b) $$\n",
    "\n",
    "So, the expression becomes:  \n",
    "$$ -\\frac{1}{2} \\left[ (w - A^{-1} b)^T A (w - A^{-1} b) - b^T A^{-1} b \\right] $$\n",
    "\n",
    "### Simplifying\n",
    "Let $ A = \\beta \\Phi^T \\Phi + \\alpha I $ and $ b = \\beta \\Phi^T t $. Then:  \n",
    "$ A^{-1} b = (\\beta \\Phi^T \\Phi + \\alpha I)^{-1} (\\beta \\Phi^T t) \\equiv m_N $  \n",
    "$ A^{-1} = S_N = (\\beta \\Phi^T \\Phi + \\alpha I)^{-1} $  \n",
    "\n",
    "Thus, the posterior mean and covariance are:  \n",
    "$ m_N = S_N (\\beta \\Phi^T t) $  \n",
    "$ S_N = (\\beta \\Phi^T \\Phi + \\alpha I)^{-1} $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50924d-b3c1-4034-b823-222b24adc05e",
   "metadata": {},
   "source": [
    "# Integrating the Product of Two Gaussians\n",
    "\n",
    "Now, we want to integrate the product $p(t \\mid x, w) \\cdot p(w \\mid x_{\\text{train}}, t_{\\text{train}})$, where both distributions are Gaussians.\n",
    "\n",
    "This is a standard result in Gaussian integration. When you multiply two Gaussians and integrate out $w$, the result is another Gaussian in terms of $t$, and its mean and variance can be derived analytically.\n",
    "\n",
    "## Predictive Distribution\n",
    "\n",
    "The predictive distribution $p(t \\mid x, x_{\\text{train}}, t_{\\text{train}})$ is also a Gaussian:\n",
    "$$\n",
    "p(t \\mid x, x_{\\text{train}}, t_{\\text{train}}) = \\mathcal{N}(t \\mid m(x), s^2(x))\n",
    "$$\n",
    "\n",
    "We will now derive the predictive mean $m(x)$ and the predictive variance $s^2(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Deriving the Predictive Mean $m(x)$\n",
    "\n",
    "The mean of the resulting Gaussian after marginalizing over $w$ is:\n",
    "$$\n",
    "m(x) = \\mathbb{E}_{p(w \\mid x_{\\text{train}}, t_{\\text{train}})} \\left[ \\phi(x)^T w \\right]\n",
    "$$\n",
    "\n",
    "Using the linearity of expectation:\n",
    "$$\n",
    "m(x) = \\phi(x)^T \\, \\mathbb{E}[w]\n",
    "$$\n",
    "\n",
    "Since the posterior mean of $w$ is $m_N$, we have:\n",
    "$$\n",
    "m(x) = \\phi(x)^T m_N\n",
    "$$\n",
    "\n",
    "Substitute $m_N = \\beta S_N \\Phi^T t_{\\text{train}}$:\n",
    "$$\n",
    "m(x) = \\phi(x)^T (\\beta S_N \\Phi^T t_{\\text{train}})\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "$$\n",
    "m(x) = \\beta \\, \\phi(x)^T S_N \\Phi^T t_{\\text{train}}\n",
    "$$\n",
    "\n",
    "Thus, the predictive mean depends on the basis functions $\\phi(x)$, the posterior covariance $S_N$, and the training targets $t_{\\text{train}}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Deriving the Predictive Variance $s^2(x)$\n",
    "\n",
    "The predictive variance $s^2(x)$ has two contributions:\n",
    "\n",
    "1. The noise variance $\\beta^{-1}$, which reflects the inherent noise in the observations.\n",
    "2. The uncertainty due to $w$, which comes from marginalizing over $w$ using its posterior distribution.\n",
    "\n",
    "The formula for the predictive variance is:\n",
    "$$\n",
    "s^2(x) = \\beta^{-1} + \\text{Var}_{p(w \\mid x_{\\text{train}}, t_{\\text{train}})} \\left[ \\phi(x)^T w \\right]\n",
    "$$\n",
    "\n",
    "Using the variance formula for a linear transformation of a Gaussian random variable:\n",
    "$$\n",
    "s^2(x) = \\beta^{-1} + \\phi(x)^T S_N \\phi(x)\n",
    "$$\n",
    "\n",
    "Here, $\\phi(x)^T S_N \\phi(x)$ is the contribution to the variance due to the uncertainty in the parameters $w$.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Result: Predictive Mean and Variance\n",
    "\n",
    "Thus, we have derived the mean and variance of the predictive distribution $p(t \\mid x, x_{\\text{train}}, t_{\\text{train}})$:\n",
    "\n",
    "**Predictive Mean:**\n",
    "$$\n",
    "m(x) = \\beta \\, \\phi(x)^T S_N \\Phi^T t_{\\text{train}}\n",
    "$$\n",
    "\n",
    "**Predictive Variance:**\n",
    "$$\n",
    "s^2(x) = \\beta^{-1} + \\phi(x)^T S_N \\phi(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6c16-4e43-4f55-a640-c5f00e25e60a",
   "metadata": {},
   "source": [
    "![image4](image4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d8788-665a-419c-8a54-6d70c17785cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
