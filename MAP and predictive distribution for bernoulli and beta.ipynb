{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9ec5fb-fa90-4b48-890a-13efde6628e9",
   "metadata": {},
   "source": [
    "# Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "The Maximum A Posteriori (MAP) estimate is a Bayesian approach to parameter estimation. It finds the value of the parameter $p$ that maximizes the posterior distribution $P(p \\mid \\text{data})$. Unlike Maximum Likelihood Estimation (MLE), which only considers the likelihood, MAP incorporates prior knowledge about the parameter through the prior distribution.\n",
    "\n",
    "For the Bernoulli-Beta model, we will derive the MAP estimate for $p$ and compare it with MLE.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Derivation of MAP for Bernoulli-Beta Model\n",
    "\n",
    "### 1. Posterior Distribution\n",
    "\n",
    "From earlier derivations, the posterior distribution for $p$ given the data is:\n",
    "\n",
    "$$\n",
    "P(p \\mid \\text{data}) \\propto P(\\text{data} \\mid p) P(p),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(\\text{data} \\mid p) = p^s (1 - p)^f$ is the likelihood,\n",
    "- $P(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha - 1} (1 - p)^{\\beta - 1}$ is the Beta prior.\n",
    "\n",
    "Substitute these into the posterior:\n",
    "\n",
    "$$\n",
    "P(p \\mid \\text{data}) \\propto p^s (1 - p)^f \\cdot p^{\\alpha - 1} (1 - p)^{\\beta - 1}.\n",
    "$$\n",
    "\n",
    "Combine terms:\n",
    "\n",
    "$$\n",
    "P(p \\mid \\text{data}) \\propto p^{s + \\alpha - 1} (1 - p)^{f + \\beta - 1}.\n",
    "$$\n",
    "\n",
    "This is proportional to a Beta distribution:\n",
    "\n",
    "$$\n",
    "P(p \\mid \\text{data}) \\sim \\text{Beta}(\\alpha + s, \\beta + f).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Objective: Maximize the Posterior\n",
    "\n",
    "To find the MAP estimate, we maximize the posterior distribution $P(p \\mid \\text{data})$.  \n",
    "Equivalently, we can maximize the log-posterior for computational simplicity:\n",
    "\n",
    "$$\n",
    "\\ell(p) = \\ln P(p \\mid \\text{data}) = \\ln \\left[ p^{s + \\alpha - 1} (1 - p)^{f + \\beta - 1} \\right].\n",
    "$$\n",
    "\n",
    "Simplify using logarithmic properties:\n",
    "\n",
    "$$\n",
    "\\ell(p) = (s + \\alpha - 1) \\ln p + (f + \\beta - 1) \\ln (1 - p).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Maximize the Log-Posterior\n",
    "\n",
    "Take the derivative of $\\ell(p)$ with respect to $p$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dp} \\ell(p) = \\frac{s + \\alpha - 1}{p} - \\frac{f + \\beta - 1}{1 - p}.\n",
    "$$\n",
    "\n",
    "Set the derivative equal to zero to find the critical points:\n",
    "\n",
    "$$\n",
    "\\frac{s + \\alpha - 1}{p} = \\frac{f + \\beta - 1}{1 - p}.\n",
    "$$\n",
    "\n",
    "Cross-multiply:\n",
    "\n",
    "$$\n",
    "(s + \\alpha - 1)(1 - p) = (f + \\beta - 1)p.\n",
    "$$\n",
    "\n",
    "Expand and rearrange:\n",
    "\n",
    "$$\n",
    "s + \\alpha - 1 - (s + \\alpha - 1)p = (f + \\beta - 1)p.\n",
    "$$\n",
    "\n",
    "Combine the terms involving $p$:\n",
    "\n",
    "$$\n",
    "s + \\alpha - 1 = (s + \\alpha - 1 + f + \\beta - 1)p.\n",
    "$$\n",
    "\n",
    "Solve for $p$:\n",
    "\n",
    "$$\n",
    "p = \\frac{s + \\alpha - 1}{s + f + \\alpha + \\beta - 2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. MAP Estimate\n",
    "\n",
    "The MAP estimate of $p$ is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{MAP}} = \\frac{s + \\alpha - 1}{n + \\alpha + \\beta - 2},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $s = \\sum_{i=1}^n x_i$ is the total number of successes,\n",
    "- $f = n - s$ is the total number of failures,\n",
    "- $\\alpha$ and $\\beta$ are the parameters of the Beta prior.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Comparison of MAP and MLE\n",
    "\n",
    "**MLE Estimate**:  \n",
    "The MLE estimate for $p$ is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{MLE}} = \\frac{s}{n}.\n",
    "$$\n",
    "\n",
    "**MAP Estimate**:  \n",
    "The MAP estimate incorporates the prior information:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{MAP}} = \\frac{s + \\alpha - 1}{n + \\alpha + \\beta - 2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Prior Influence**: The terms $\\alpha - 1$ and $\\beta - 1$ in the numerator and denominator reflect the influence of the prior.\n",
    "- **Regularization**: The MAP estimate can be viewed as a regularized version of the MLE estimate, where the prior acts as a smoothing factor to prevent overfitting to small datasets.\n",
    "- **Behavior with Small Data**:  \n",
    "  When $n$ is small, the MAP estimate is closer to the prior mean $\\frac{\\alpha}{\\alpha + \\beta}$ than the MLE estimate.  \n",
    "  As $n \\to \\infty$, the influence of the prior diminishes, and $\\hat{p}_{\\text{MAP}} \\to \\hat{p}_{\\text{MLE}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210be160-25d5-4958-bd79-cfaa762f7a10",
   "metadata": {},
   "source": [
    "# Predictive Distribution in Bayesian Inference  \n",
    "\n",
    "The predictive distribution is a key concept in Bayesian statistics. It allows us to make predictions about new, unseen data based on the observed data and our prior beliefs. Specifically, it integrates over the uncertainty in the parameter $p$ (e.g., the probability of success in a Bernoulli trial) by using the posterior distribution.  \n",
    "\n",
    "In this discussion, we'll derive the posterior predictive distribution for the Bernoulli-Beta model and explore its interpretation and applications.  \n",
    "\n",
    "## Posterior Predictive Distribution: Definition  \n",
    "\n",
    "The posterior predictive distribution gives the probability of observing a new data point $X_{\\text{new}}$ given the observed data. Mathematically:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} \\mid \\text{data}) = \\int_{0}^{1} P(X_{\\text{new}} \\mid p) P(p \\mid \\text{data}) dp.  \n",
    "$$  \n",
    "\n",
    "where:  \n",
    "\n",
    "- $P(X_{\\text{new}} \\mid p)$ is the likelihood of the new observation given $p$,  \n",
    "- $P(p \\mid \\text{data})$ is the posterior distribution of $p$ after observing the data.  \n",
    "\n",
    "For the Bernoulli-Beta model:  \n",
    "\n",
    "- $P(X_{\\text{new}} \\mid p)$ is Bernoulli: $P(X_{\\text{new}} = 1 \\mid p) = p$ and $P(X_{\\text{new}} = 0 \\mid p) = 1 - p$,  \n",
    "- $P(p \\mid \\text{data})$ is Beta: $P(p \\mid \\text{data}) \\sim \\text{Beta}(\\alpha + s, \\beta + f)$.  \n",
    "\n",
    "---\n",
    "\n",
    "## Derivation of the Posterior Predictive Distribution  \n",
    "\n",
    "### Step 1: Write the Predictive Distribution  \n",
    "\n",
    "The predictive distribution is:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} \\mid \\text{data}) = \\int_{0}^{1} P(X_{\\text{new}} \\mid p) P(p \\mid \\text{data}) dp.  \n",
    "$$  \n",
    "\n",
    "Substitute the Bernoulli likelihood $P(X_{\\text{new}} \\mid p)$ and the Beta posterior $P(p \\mid \\text{data})$:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} \\mid \\text{data}) = \\int_{0}^{1} \\left[ p^{X_{\\text{new}}} (1 - p)^{1 - X_{\\text{new}}} \\right] \\cdot \\frac{p^{\\alpha + s - 1} (1 - p)^{\\beta + f - 1}}{B(\\alpha + s, \\beta + f)} dp.  \n",
    "$$  \n",
    "\n",
    "Combine terms:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} \\mid \\text{data}) = \\frac{1}{B(\\alpha + s, \\beta + f)} \\int_{0}^{1} p^{\\alpha + s + X_{\\text{new}} - 1} (1 - p)^{\\beta + f + (1 - X_{\\text{new}}) - 1} dp.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Recognize the Beta Integral  \n",
    "\n",
    "The integral has the form of a Beta function:  \n",
    "\n",
    "$$  \n",
    "\\int_{0}^{1} p^{a - 1} (1 - p)^{b - 1} dp = B(a, b).  \n",
    "$$  \n",
    "\n",
    "Here:  \n",
    "- $a = \\alpha + s + X_{\\text{new}}$,  \n",
    "- $b = \\beta + f + (1 - X_{\\text{new}})$.  \n",
    "\n",
    "Thus:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} \\mid \\text{data}) = \\frac{B(\\alpha + s + X_{\\text{new}}, \\beta + f + (1 - X_{\\text{new}}))}{B(\\alpha + s, \\beta + f)}.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Simplify for Specific Cases  \n",
    "\n",
    "**Case 1:** $X_{\\text{new}} = 1$ (Success)  \n",
    "\n",
    "If $X_{\\text{new}} = 1$:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} = 1 \\mid \\text{data}) = \\frac{B(\\alpha + s + 1, \\beta + f)}{B(\\alpha + s, \\beta + f)}.  \n",
    "$$  \n",
    "\n",
    "Using the property of the Beta function:  \n",
    "\n",
    "$$  \n",
    "B(a, b) = \\frac{\\Gamma(a) \\Gamma(b)}{\\Gamma(a + b)},  \n",
    "$$  \n",
    "\n",
    "we can rewrite this as:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} = 1 \\mid \\text{data}) = \\frac{\\Gamma(\\alpha + s + 1) \\Gamma(\\beta + f)}{\\Gamma(\\alpha + s + \\beta + f + 1)} \\cdot \\frac{\\Gamma(\\alpha + s + \\beta + f)}{\\Gamma(\\alpha + s) \\Gamma(\\beta + f)}.  \n",
    "$$  \n",
    "\n",
    "Simplify using $\\Gamma(n + 1) = n \\Gamma(n)$:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} = 1 \\mid \\text{data}) = \\frac{\\alpha + s}{\\alpha + s + \\beta + f}.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "**Case 2:** $X_{\\text{new}} = 0$ (Failure)  \n",
    "\n",
    "If $X_{\\text{new}} = 0$:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} = 0 \\mid \\text{data}) = \\frac{\\beta + f}{\\alpha + s + \\beta + f}.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## Final Result: Posterior Predictive Distribution  \n",
    "\n",
    "The posterior predictive distribution for $X_{\\text{new}}$ is:  \n",
    "\n",
    "$$  \n",
    "P(X_{\\text{new}} = 1 \\mid \\text{data}) = \\frac{\\alpha + s}{\\alpha + s + \\beta + f},  \n",
    "$$  \n",
    "$$  \n",
    "P(X_{\\text{new}} = 0 \\mid \\text{data}) = \\frac{\\beta + f}{\\alpha + s + \\beta + f}.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind the Predictive Distribution  \n",
    "\n",
    "1. **Weighted Average of Prior and Data:**  \n",
    "   The predictive probabilities are weighted averages of the prior mean $\\frac{\\alpha}{\\alpha + \\beta}$ and the observed proportion of successes $\\frac{s}{n}$.  \n",
    "   - As $n \\to \\infty$, the influence of the prior diminishes, and the predictive probabilities converge to the observed proportion.  \n",
    "\n",
    "2. **Uncertainty Quantification:**  \n",
    "   The predictive distribution incorporates the uncertainty in $p$ by integrating over the posterior distribution. This makes it more robust than simply using the posterior mean.  \n",
    "\n",
    "3. **Applications:**  \n",
    "   The predictive distribution is useful for making probabilistic predictions about future outcomes.  \n",
    "   - For example, in medical trials, it can predict the probability of success for the next patient based on previous observations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53890f2e-0077-40d6-95e9-290a8404749e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
