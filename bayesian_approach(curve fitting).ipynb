{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b358cd9-b5e9-43ae-8191-4cf644aaf159",
   "metadata": {},
   "source": [
    "# Bayesian Approach to Curve Fitting\n",
    "\n",
    "The goal in the curve fitting problem is to be able to make predictions for the target variable $ t $ given some new value of the input variable $ x $, on the basis of a set of training data comprising $ N $ input values $ \\mathbf{x} = (x_1, \\ldots, x_N)^T $ and their corresponding target values $\\mathbf{t} = (t_1, \\ldots, t_N)^T $.\n",
    "\n",
    "We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of $ x $, the corresponding value of $ t $ has a Gaussian distribution with a mean equal to the value $ y(x, w)$ of the polynomial curve. Thus, we have:\n",
    "\n",
    "$$\n",
    "p(t | x, w, \\beta) = \\mathcal{N}(t | y(x, w), \\beta^{-1}) \\tag{1.1}\n",
    "$$\n",
    "\n",
    "We now use the training data $ \\{\\mathbf{x}, \\mathbf{t}\\} $ to determine the values of the unknown parameters $w$ and $ \\beta $ by maximum likelihood.\n",
    "\n",
    "If the data are assumed to be drawn independently from the distribution \\( (1.1) \\), then the likelihood function is given by:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{x}, w, \\beta) = \\prod_{n=1}^N \\mathcal{N}(t_n | y(x_n, w), \\beta^{-1}) \\tag{1.2}\n",
    "$$\n",
    "\n",
    "As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b680ffe-8fb6-4e36-9099-9695c7a3cd87",
   "metadata": {},
   "source": [
    "![image 3](image3.png)\n",
    "\n",
    "\n",
    "image taken from bishops book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b04444-4b8b-4f39-bc67-040495428f17",
   "metadata": {},
   "source": [
    "# Derivation of the Log-Likelihood for a Normally Distributed Random Variable  \n",
    "\n",
    "For a normally distributed random variable $t_n \\sim \\mathcal{N}(y(x_n, w), \\beta^{-1})$, the probability density function is given by:  \n",
    "\n",
    "$$  \n",
    "p(t_n \\mid y(x_n, w), \\beta^{-1}) = \\left( \\frac{\\beta}{2\\pi} \\right)^{\\frac{1}{2}} \\exp\\left\\{ -\\frac{\\beta}{2} \\left( t_n - y(x_n, w) \\right)^2 \\right\\}.  \n",
    "$$  \n",
    "\n",
    "This is the Gaussian density function with mean $y(x_n, w)$ and precision $\\beta$ (the precision is the inverse of the variance).  \n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Express the Likelihood Function Explicitly  \n",
    "\n",
    "Using the Gaussian form for each $p(t_n \\mid y(x_n, w), \\beta^{-1})$ and substituting it into the likelihood function $p(t \\mid x, w, \\beta)$, we get:  \n",
    "\n",
    "$$  \n",
    "p(t \\mid x, w, \\beta) = \\prod_{n=1}^N \\left( \\frac{\\beta}{2\\pi} \\right)^{\\frac{1}{2}} \\exp\\left\\{ -\\frac{\\beta}{2} \\left( t_n - y(x_n, w) \\right)^2 \\right\\}.  \n",
    "$$  \n",
    "\n",
    "This is the expanded form of the likelihood.  \n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Take the Logarithm of the Likelihood Function  \n",
    "\n",
    "Since it is generally easier to maximize the log-likelihood (logarithm of the likelihood function), we take the logarithm on both sides:  \n",
    "\n",
    "$$  \n",
    "\\ln p(t \\mid x, w, \\beta) = \\sum_{n=1}^N \\left[ \\frac{1}{2} \\ln \\left( \\frac{\\beta}{2\\pi} \\right) - \\frac{\\beta}{2} \\left( t_n - y(x_n, w) \\right)^2 \\right].  \n",
    "$$  \n",
    "\n",
    "Now, split the terms inside the summation:  \n",
    "\n",
    "$$  \n",
    "\\ln p(t \\mid x, w, \\beta) = \\frac{N}{2} \\ln \\left( \\frac{\\beta}{2\\pi} \\right) - \\frac{\\beta}{2} \\sum_{n=1}^N \\left( t_n - y(x_n, w) \\right)^2.  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Simplify the Terms  \n",
    "\n",
    "Expand the first term:  \n",
    "\n",
    "$$  \n",
    "\\frac{N}{2} \\ln \\left( \\frac{\\beta}{2\\pi} \\right) = \\frac{N}{2} \\ln \\beta - \\frac{N}{2} \\ln (2\\pi).  \n",
    "$$  \n",
    "\n",
    "The second term remains as it is:  \n",
    "\n",
    "$$  \n",
    "-\\frac{\\beta}{2} \\sum_{n=1}^N \\left( t_n - y(x_n, w) \\right)^2.  \n",
    "$$  \n",
    "\n",
    "So, we can write the final expression for the log-likelihood as:  \n",
    "\n",
    "$$  \n",
    "\\ln p(t \\mid x, w, \\beta) = -\\frac{\\beta}{2} \\sum_{n=1}^N \\left( t_n - y(x_n, w) \\right)^2 + \\frac{N}{2} \\ln \\beta - \\frac{N}{2} \\ln (2\\pi).  \n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7196819-d719-4880-99cb-bac1d64d5873",
   "metadata": {},
   "source": [
    "# Maximizing the Log-Likelihood with Respect to $w$\n",
    "\n",
    "To find the Maximum Likelihood Estimator (MLE) for $w$, we focus on the term involving $y(x_n, w)$, which depends on $w$. Let’s differentiate the log-likelihood with respect to $w$ and set the derivative equal to zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Focus on the $w$-Dependent Part  \n",
    "\n",
    "The $w$-dependent part of the log-likelihood is:  \n",
    "\n",
    "$$  \n",
    "-\\frac{\\beta}{2} \\sum_{n=1}^N \\left( y(x_n, w) - t_n \\right)^2.  \n",
    "$$  \n",
    "\n",
    "This is equivalent to minimizing the sum of squared errors (SSE), which is a common objective in regression.  \n",
    "\n",
    "---\n",
    "\n",
    "## Minimizing the SSE  \n",
    "\n",
    "To maximize the log-likelihood, we minimize the SSE:  \n",
    "\n",
    "$$  \n",
    "\\min_w \\sum_{n=1}^N \\left( y(x_n, w) - t_n \\right)^2.  \n",
    "$$  \n",
    "\n",
    "This is a classic least squares problem. Solving this will give the optimal weights $w^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780b8d0-24f0-4712-a1a9-b952c4ddb771",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We want to minimize the **sum of squared errors (SSE)**:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\sum_{n=1}^{N} \\left( w^T \\phi(x_n) - t_n \\right)^2\n",
    "$$\n",
    "\n",
    "### Where:\n",
    "- $w $ is the **weight vector** (parameters to be optimized),  \n",
    "- $\\phi(x_n) $ is the **feature vector** corresponding to the input $ x_n $,  \n",
    "- $ t_n $ is the **target (output)** corresponding to \\( x_n \\),  \n",
    "- $ w^T \\phi(x_n) $ represents the **predicted output** for input $ x_n $.  \n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Design Matrix $ \\Phi $\n",
    "\n",
    "The matrix $ \\Phi $ is called the **design matrix**, and each row of $ \\Phi $ corresponds to a feature vector $ \\phi(x_n) $, derived from the input data $ x_n $.\n",
    "\n",
    "### Step 1: What is $ \\phi(x_n) $ ?\n",
    "The vector $ \\phi(x_n) $ is the **feature vector** associated with the input $ x_n $ .\n",
    "\n",
    "Suppose we have a set of $ N $ data points $ { x_1, x_2, \\ldots, x_N } $ , and each $ x_n $ is mapped to a feature vector $ \\phi(x_n) $, where:\n",
    "\n",
    "$$\n",
    "\\phi(x_n) = \n",
    "\\begin{bmatrix}\n",
    "\\phi_1(x_n) \\\\\n",
    "\\phi_2(x_n) \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_M(x_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $ M $ components (features). This means that $ \\phi(x_n) $ is a vector of length $ M $.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Building the Design Matrix  $ \\Phi $\n",
    "Now, we stack these $ N $ feature vectors (each of length $ M $) on top of each other to form the matrix $ \\Phi $:\n",
    "\n",
    "$$\n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "\\phi(x_1)^T \\\\\n",
    "\\phi(x_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "\\phi(x_N)^T \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_M(x_1) \\\\\n",
    "\\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_M(x_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_1(x_N) & \\phi_2(x_N) & \\ldots & \\phi_M(x_N) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Dimensions of $ \\Phi $\n",
    "From this construction, it is clear that:\n",
    "- There are  N  rows in  $ \\Phi $, each corresponding to one input $ x_n $,\n",
    "- There are M  columns in $ \\Phi $, each corresponding to a feature $ \\phi_j(x_n) $ in the feature vector.\n",
    "\n",
    "Thus, the matrix $ \\Phi $  has dimensions $ N \\times M $, where:\n",
    "- N : Number of data points (inputs),  \n",
    "- M : Number of features in each feature vector.  \n",
    "\n",
    "---\n",
    "\n",
    "### Rewrite in Matrix Form  \n",
    "To solve this more compactly, we can write the objective function in **matrix form**. Define:\n",
    "- $ Phi $(the design matrix) as a matrix where each row is a feature vector $ phi(x_n)^T $. Thus:\n",
    "\n",
    "  $$\n",
    "  \\Phi = \n",
    "  \\begin{bmatrix}\n",
    "  \\phi(x_1)^T \\\\\n",
    "  \\phi(x_2)^T \\\\\n",
    "  \\vdots \\\\\n",
    "  \\phi(x_N)^T\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  If $ phi(x_n) $ has $ M $ components, then $ \\Phi $ is an $ N \\times M $ matrix.\n",
    "\n",
    "- $ t $ as the **target vector**:\n",
    "\n",
    "  $$\n",
    "  t = \n",
    "  \\begin{bmatrix}\n",
    "  t_1 \\\\\n",
    "  t_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  t_N\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- $ y $ as the vector of predictions $ y = \\Phi w $, where:\n",
    "\n",
    "  $$\n",
    "  y = \n",
    "  \\begin{bmatrix}\n",
    "  w^T \\phi(x_1) \\\\\n",
    "  w^T \\phi(x_2) \\\\\n",
    "  \\vdots \\\\\n",
    "  w^T \\phi(x_N)\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "Now, the objective function becomes:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\| \\Phi w - t \\|^2\n",
    "$$\n",
    "\n",
    "where $ |\\cdot \\|^2 $ denotes the squared Euclidean norm.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Expand the Squared Term\n",
    "Expand the squared norm:\n",
    "\n",
    "$$\n",
    "\\| \\Phi w - t \\|^2 = (\\Phi w - t)^T (\\Phi w - t)\n",
    "$$\n",
    "\n",
    "Expanding this expression:\n",
    "\n",
    "$$\n",
    "(\\Phi w - t)^T (\\Phi w - t) = w^T \\Phi^T \\Phi w - 2 t^T \\Phi w + t^T t\n",
    "$$\n",
    "\n",
    "This is a quadratic function of w .\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Differentiate with Respect to \\( w \\)\n",
    "To find the value of  w  that minimizes the objective, differentiate the above expression with respect to  w , and set the derivative to zero.\n",
    "\n",
    "Differentiate term by term:\n",
    "\n",
    "- $ \\frac{\\partial}{\\partial w} \\left( w^T \\Phi^T \\Phi w \\right) = 2 \\Phi^T \\Phi w $\n",
    "- $ \\frac{\\partial}{\\partial w} \\left( - 2 t^T \\Phi w \\right) = - 2 \\Phi^T t $\n",
    "\n",
    "The term $ t^T t $ is constant and vanishes after differentiation.\n",
    "\n",
    "Thus, the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} \\left( \\| \\Phi w - t \\|^2 \\right) = 2 \\Phi^T \\Phi w - 2 \\Phi^T t\n",
    "$$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$\n",
    "2 \\Phi^T \\Phi w - 2 \\Phi^T t = 0\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\Phi^T \\Phi w = \\Phi^T t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Solve for $ w^*$\n",
    "Assuming that $ \\Phi^T \\Phi $ is invertible, we can solve for the optimal weight vector $ w^* $:\n",
    "\n",
    "$$\n",
    "w^* = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Interpretation\n",
    "This is the **closed-form solution** for the least squares problem. It shows that the optimal weights  $w^*$ depend on:\n",
    "- $ \\Phi^T \\Phi $, which captures the relationships between the features,\n",
    "- $ \\Phi^T t $, which captures the relationship between the features and the targets $ t_n $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c621ad-292e-49b8-bf56-ddd1d5831e01",
   "metadata": {},
   "source": [
    "# Maximizing with Respect to $\\beta $\n",
    "\n",
    "Now, we maximize the **log-likelihood** with respect to \\( \\beta \\).\n",
    "\n",
    "### Focus on the \\( \\beta \\)-Dependent Part:\n",
    "The \\( \\beta \\)-dependent terms in the log-likelihood are:\n",
    "\n",
    "$$\n",
    "\\ln p(t \\mid x, w, \\beta) = -\\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2 + \\frac{N}{2} \\ln \\beta - \\frac{N}{2} \\ln (2 \\pi).\n",
    "$$\n",
    "\n",
    "To find the value of \\( \\beta \\) that maximizes this expression, we will differentiate it with respect to \\( \\beta \\) and set the derivative to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\ln p(t \\mid x, w^*, \\beta) = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Differentiate Term by Term  \n",
    "\n",
    "### 1. Differentiate the First Term:\n",
    "Let’s differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\left[ - \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2 \\right].\n",
    "$$\n",
    "\n",
    "Since only $ \\beta $ varies, and the summation inside is independent of $ \\beta $, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\left[ - \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2 \\right] = - \\frac{1}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Differentiate the Second Term:\n",
    "Now, differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\left( \\frac{N}{2} \\ln \\beta \\right).\n",
    "$$\n",
    "\n",
    "Using the derivative of \\( \\ln \\beta \\), we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\left( \\frac{N}{2} \\ln \\beta \\right) = \\frac{N}{2 \\beta}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Differentiate the Third Term:\n",
    "The third term, $ - \\frac{N}{2} \\ln (2 \\pi) $, is constant with respect to $ \\beta $, so its derivative is 0.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Set the Derivative to Zero\n",
    "Now, we set the derivative of the entire expression to 0:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2 + \\frac{N}{2 \\beta} = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Simplify and Solve for $ \\beta $\n",
    "Rearranging terms:\n",
    "\n",
    "$$\n",
    "\\frac{N}{2 \\beta} = \\frac{1}{2} \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2.\n",
    "$$\n",
    "\n",
    "Multiplying both sides by \\( 2 \\beta \\):\n",
    "\n",
    "$$\n",
    "N = \\beta \\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2.\n",
    "$$\n",
    "\n",
    "Finally, solve for $ \\beta $:\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{N}{\\sum_{n=1}^{N} \\left( y(x_n, w^*) - t_n \\right)^2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation:\n",
    "This is the **Maximum Likelihood Estimate (MLE)** for \\( \\beta \\), and it represents the **precision** (the inverse of variance) in terms of the sum of squared residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8ab38-4398-44f3-92a1-b6223657e322",
   "metadata": {},
   "source": [
    "we now take a step toward a **Bayesian approach** by introducing a **prior distribution** over the polynomial coefficients w . This allows us to incorporate prior knowledge into the model, which helps in regularizing the solution and reducing overfitting. \n",
    "\n",
    "---\n",
    "\n",
    "## Prior Distribution\n",
    "\n",
    "Let’s define the prior distribution over  w . For simplicity, we assume a **Gaussian prior**, which has the following form:\n",
    "\n",
    "$\n",
    "p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I),\n",
    "$\n",
    "where:\n",
    "- $( \\mathcal{N}(w \\mid 0, \\alpha^{-1} I) )$ represents a **multivariate Gaussian distribution** with:\n",
    "  - Mean vector $ 0 $,\n",
    "  - Covariance matrix $ \\alpha^{-1} I $, where $ I $ is the identity matrix.\n",
    "-  $\\alpha $ is a positive parameter called the **precision** (the inverse of variance).\n",
    "- $ w $ is a vector of polynomial coefficients, and for an $ M $-th order polynomial, $ w $ has $ M+1$  elements.\n",
    "\n",
    "This Gaussian distribution can be expanded as follows:\n",
    "\n",
    "$$\n",
    "p(w \\mid \\alpha) = \\left( \\frac{\\alpha}{2 \\pi} \\right)^{(M+1)/2} \\exp \\left( - \\frac{\\alpha}{2} w^\\top w \\right).\n",
    "$$\n",
    "\n",
    "### Interpretation:\n",
    "- $ \\alpha $ is called a **hyperparameter**, as it controls the spread (or precision) of the prior distribution over the polynomial coefficients $w $.\n",
    "- The term $ w^\\top w $ represents the **L2 norm** (squared Euclidean distance) of the vector $ w $.\n",
    "\n",
    "---\n",
    "\n",
    "## Posterior Distribution\n",
    "\n",
    "Using **Bayes’ Theorem**, we can now update our prior beliefs about $ w $ after observing data $ \\{x, t\\} $. Bayes’ Theorem is given by:\n",
    "\n",
    "$$\n",
    "p(w \\mid x, t, \\alpha, \\beta) \\propto p(t \\mid x, w, \\beta) p(w \\mid \\alpha),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ p(w \\mid x, t, \\alpha, \\beta) $ is the **posterior distribution**, representing our updated belief about  w  after observing the data.\n",
    "- $ p(t \\mid x, w, \\beta) $ is the **likelihood function**, representing the probability of observing the data given the parameters w .\n",
    "- $ p(w \\mid \\alpha) $ is the **prior distribution**, as defined above.\n",
    "- $ \\beta $ is another precision parameter that controls the noise in the likelihood function.\n",
    "\n",
    "---\n",
    "\n",
    "## Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "To find the optimal value of  w , we want to maximize the posterior distribution  $ p(w \\mid x, t, \\alpha, \\beta) $. This approach is called **Maximum A Posteriori (MAP)** estimation.\n",
    "\n",
    "Rather than maximizing the posterior directly, we can simplify the problem by taking the **negative logarithm** of the posterior (since logarithms are monotonically increasing, this will preserve the optimal value). This turns the product in Bayes' Theorem into a sum:\n",
    "\n",
    "$$\n",
    "\\ln p(w \\mid x, t, \\alpha, \\beta) = \\ln p(t \\mid x, w, \\beta) + \\ln p(w \\mid \\alpha) + \\text{constant}.\n",
    "$$\n",
    "\n",
    "Since maximizing the log-posterior is equivalent to minimizing the negative log-posterior, we get:\n",
    "\n",
    "$$\n",
    "\\text{Minimize:} \\quad - \\ln p(w \\mid x, t, \\alpha, \\beta).\n",
    "$$\n",
    "\n",
    "Using the expressions for the likelihood $ p(t \\mid x, w, \\beta) $ and the prior $ p(w \\mid \\alpha) $, we can write the negative log-posterior as:\n",
    "\n",
    "$$\n",
    "-\\ln p(w \\mid x, t, \\alpha, \\beta) = \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w) - t_n \\right)^2 + \\frac{\\alpha}{2} w^\\top w + \\text{constant}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation and Connection to Regularization\n",
    "\n",
    "Let’s examine this result in detail:\n",
    "\n",
    "1. The first term:\n",
    "\n",
    "   $\n",
    "   \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w) - t_n \\right)^2,\n",
    "   $\n",
    "   corresponds to the **sum-of-squares error** we encountered earlier. This term measures how well the polynomial $ y(x, w)$ fits the observed data $ \\{t_n\\} $.\n",
    "\n",
    "2. The second term:\n",
    "\n",
    "   $\n",
    "   \\frac{\\alpha}{2} w^\\top w,\n",
    "   $\n",
    "   represents a **regularization term**, which penalizes large values of the polynomial coefficients  w . This prevents overfitting by encouraging smaller, smoother coefficients.\n",
    "\n",
    "Thus, maximizing the posterior distribution is equivalent to minimizing the following **regularized sum-of-squares error function**:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{\\beta}{2} \\sum_{n=1}^{N} \\left( y(x_n, w) - t_n \\right)^2 + \\frac{\\alpha}{2} w^\\top w.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization Parameter $ \\lambda $\n",
    "\n",
    "Notice that this objective function is identical to the regularized sum-of-squares error function we encountered earlier:\n",
    "\n",
    "$$\n",
    "E(w) = \\sum_{n=1}^{N} \\left( y(x_n, w) - t_n \\right)^2 + \\lambda w^\\top w,\n",
    "$$\n",
    "where $ \\lambda = \\frac{\\alpha}{\\beta} $ is the **regularization parameter**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c328a-1b0e-4ed0-9528-8ba015a5b475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
