{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d2c0a8-3d7d-4a3a-a8cf-2fb718d294e9",
   "metadata": {},
   "source": [
    "# Kullback-Leibler (KL) Divergence\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Kullback-Leibler (KL) Divergence** is a fundamental concept in information theory and machine learning. It is used to measure how one probability distribution differs from another. \n",
    "\n",
    "Unlike distance metrics like Euclidean distance, KL divergence is not symmetric, meaning that the divergence from distribution \\( P \\) to distribution \\( Q \\) is not the same as the divergence from \\( Q \\) to \\( P \\). It is often referred to as the **relative entropy**.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition of KL Divergence\n",
    "\n",
    "Mathematically, the KL divergence between two probability distributions \\( P(x) \\) and \\( Q(x) \\) is given by:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = \\sum_{x} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right),\n",
    "$$\n",
    "\n",
    "for discrete random variables, or\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = \\int_{-\\infty}^{\\infty} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) dx,\n",
    "$$\n",
    "\n",
    "for continuous random variables.\n",
    "\n",
    "In both cases, \\( P(x) \\) is called the **true distribution**, and \\( Q(x) \\) is called the **approximation** or **reference distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind KL Divergence\n",
    "\n",
    "The KL divergence measures the **extra information (in bits)** needed to encode data that follows the true distribution \\( P(x) \\) using a model based on the distribution \\( Q(x) \\).\n",
    "\n",
    "If \\( P \\) and \\( Q \\) are identical, then \\( D_{KL}(P \\, || \\, Q) = 0 \\), meaning there is no additional cost in using \\( Q \\) to approximate \\( P \\).\n",
    "\n",
    "If \\( P \\) and \\( Q \\) differ, the KL divergence quantifies how much information is lost when using \\( Q \\) instead of \\( P \\).\n",
    "\n",
    "---\n",
    "\n",
    "![image6](https://www.researchgate.net/profile/Duco-Veen/publication/319662351/figure/fig1/AS:614240771637255@1523457820692/KL-divergences-between-two-normal-distributions-In-this-example-p-1-is-a-standard-normal.png)\n",
    "\n",
    "## Derivation of KL Divergence (Discrete Case)\n",
    "\n",
    "Letâ€™s derive the KL divergence step by step for the discrete case.\n",
    "\n",
    "### Step 1: Expected Information Content\n",
    "\n",
    "From information theory, the amount of information (or surprise) associated with an event \\( x \\) occurring, given that it follows a distribution \\( P(x) \\), is:\n",
    "\n",
    "$$\n",
    "h(x) = -\\log P(x).\n",
    "$$\n",
    "\n",
    "If we approximate \\( P(x) \\) by another distribution \\( Q(x) \\), the information content changes to:\n",
    "\n",
    "$$\n",
    "h_Q(x) = -\\log Q(x).\n",
    "$$\n",
    "\n",
    "### Step 2: Expected Information Difference\n",
    "\n",
    "Now, consider the difference in information content when we mistakenly use \\( Q(x) \\) instead of the true distribution \\( P(x) \\):\n",
    "\n",
    "$$\n",
    "h(x) - h_Q(x) = - \\log P(x) + \\log Q(x).\n",
    "$$\n",
    "\n",
    "### Step 3: Taking the Expectation\n",
    "\n",
    "The KL divergence measures the expected difference between these information contents, weighted by the true distribution \\( P(x) \\):\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = \\sum_{x} P(x) \\left[ - \\log P(x) + \\log Q(x) \\right].\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = \\sum_{x} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Properties of KL Divergence\n",
    "\n",
    "1. **Non-Negativity**: \n",
    "   - The KL divergence is always non-negative:\n",
    "     $$\n",
    "     D_{KL}(P \\, || \\, Q) \\geq 0,\n",
    "     $$\n",
    "     with equality if and only if \\( P = Q \\).\n",
    "   \n",
    "2. **Asymmetry**:\n",
    "   - KL divergence is not symmetric:\n",
    "     $$\n",
    "     D_{KL}(P \\, || \\, Q) \\neq D_{KL}(Q \\, || \\, P).\n",
    "     $$\n",
    "     This means that switching \\( P \\) and \\( Q \\) will yield different values.\n",
    "\n",
    "3. **No True Distance Metric**:\n",
    "   - Since it is not symmetric and does not satisfy the triangle inequality, KL divergence is not a true distance metric.\n",
    "\n",
    "---\n",
    "\n",
    "## KL Divergence for Continuous Distributions\n",
    "\n",
    "For continuous random variables, KL divergence is defined using integrals instead of summations:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = \\int_{-\\infty}^{\\infty} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) dx.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "### Suppose \\( P(x) \\) and \\( Q(x) \\) are two probability distributions defined on the same sample space:\n",
    "\n",
    "- \\( P(x) = [0.4, 0.6] \\),\n",
    "- \\( Q(x) = [0.5, 0.5] \\).\n",
    "\n",
    "To compute \\( D_{KL}(P \\, || \\, Q) \\):\n",
    "\n",
    "1. \\( P(1) = 0.4 \\), \\( Q(1) = 0.5 \\),\n",
    "2. \\( P(2) = 0.6 \\), \\( Q(2) = 0.5 \\).\n",
    "\n",
    "Now, apply the formula:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) = 0.4 \\log \\left( \\frac{0.4}{0.5} \\right) + 0.6 \\log \\left( \\frac{0.6}{0.5} \\right).\n",
    "$$\n",
    "\n",
    "Calculate:\n",
    "\n",
    "- \\( \\log \\left( \\frac{0.4}{0.5} \\right) = \\log(0.8) \\approx -0.097 \\),\n",
    "- \\( \\log \\left( \\frac{0.6}{0.5} \\right) = \\log(1.2) \\approx 0.079 \\).\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) \\approx (0.4)(-0.097) + (0.6)(0.079),\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\, || \\, Q) \\approx -0.0388 + 0.0474 = 0.0086.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of KL Divergence\n",
    "\n",
    "1. **Machine Learning**:\n",
    "   - Used in classification, clustering, and generative models (e.g., Variational Autoencoders).\n",
    "   - A key component in loss functions, like Cross-Entropy Loss.\n",
    "\n",
    "2. **Information Theory**:\n",
    "   - Measures the inefficiency of assuming distribution \\( Q \\) when the true distribution is \\( P \\).\n",
    "\n",
    "3. **Bayesian Inference**:\n",
    "   - Quantifies the difference between prior and posterior distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- KL divergence measures how much one probability distribution diverges from another.\n",
    "- It is non-negative and asymmetric.\n",
    "- KL divergence plays a crucial role in machine learning, Bayesian inference, and information theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298a035-92bd-413b-be6d-b4a8b62ce43d",
   "metadata": {},
   "source": [
    "[refer this video](https://www.youtube.com/watch?v=LOwj7UxQwJ0&t=520s)\n",
    "and\n",
    "[refer to this vide aswell](https://www.youtube.com/watch?v=q0AkK8aYbLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d87b8-72e4-461f-9175-48fcdc78171a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
